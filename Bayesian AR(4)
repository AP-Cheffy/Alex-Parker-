### Following Code is Replication from the following link https://www.kaggle.com/dfoly1/bayesian-regression-blog-post. However it has been changed to Canadian Real GDP and 4 Lags is used to capture the dynamics of the system. Current Work in Progress to increase the dimenesions of the system to K Canadian Variables.
## Estimation procedure follows a Gibb Sampling approach for drawing posterior estimatons for forecasting.


## End goal is to extend the current order to include IRF and furthermore counterfactual simulations over the specified horizon. 

rm(list = ls())

library(ggplot2)
library(reshape2)
library(matrixStats)
theme_set(theme_minimal())

##############################################################################
## Load Companion Matrix 
ar_companion_matrix <- function(beta){
  if (is.matrix(beta) == FALSE){
    stop('error:beta needs to be a matrix')
  }
  #don't include constant
  k = nrow(beta) - 1
  FF <- matrix(0,nrow = k, ncol = k)
  
  #insert identity matrix 
  FF[2:k,1:(k-1)] <- diag(1, nrow = k-1, ncol = k-1)
  # dimension(beta) <- c(k+1,1)
  temp <- t(beta[2:(k+1),1:1])
  #state space companion form 
  # Insert Coeficients along top row 
  FF[1:1,1:k] <- temp
  return(FF)
  
}


## Load Data and Define Real GDP  

xdata <- read.csv("/Users/geoffreyaparker/Documents/R&Python&Matlab/R Code/Data_2GDP.csv",header = TRUE, na.strings = "")
head(xdata)
GDP <- xdata[,"GDP"]
Def  <- xdata[,"Deflator"]
Y <- data.frame(diff(log(GDP/Def))*100)


## Lags order & plus calibrate Rows
p = 4
lY = length(Y)
T1 = nrow(Y)
##############################################################################
#### Create regression matrix #### 

Cndreg <- function(data,p,constant){
  nrow <- as.numeric(dim(data)[1])
  nvar <- as.numeric(dim(data)[2])
  
  Y1 <- as.matrix(data, ncol = nvar)
  X <- embed(Y1, p+1)
  X <- X[,(nvar+1):ncol(X)]
  if(constant == TRUE){
    X <- cbind(rep(1,(nrow-p)),X)
  }
  Y = matrix(Y1[(p+1):nrow(Y1),])
  nvar2 = ncol(X)
  return = list(Y=Y,X=X,nvar2=nvar2,nrow=nrow)
}
##############################################################################
#### stability test ####
stability <- function(beta,n,l){
  FF <- matrix(0,nrow = n*1,ncol = n*1)
  #Companion Form, Skips First Row
  # Go Across to Column of Number of Variables as
  # only making identitites for 1 lag so column is p-1 lags 
  FF[(n+1):(n*1), 1:n*(l-1)] <- diag(1,nrow = n*(l-1), ncol = n*(l-1))
  
  dim(beta) <- c(n*l+1,n)
  temp <- t(beta[2:(n*1)+1,1:n])
  #state space companion form
  FF[1:n,1:(n*1)] <- temp
  ee = max(sapply(eigen(FF)$values,abs))
  S = ee > 1
  return(S)
}
##############################################################################

# Having defined both the regression matrix
# and the AR companion matrix (nxn) 
# The following six lines ex.ract the matrices and numbers of rows 
results <- Cndreg(Y,p,TRUE)
Y <- results$Y
X <- results$X
nrow <- results$nrow
nvar <- results$nvar
##############################################################
## Initialize Priors - e ~ (B_0,Sigma^2) 
B0 <- c(rep(0,p+1))  # 1 Variable, 4 Lags, and Constant so k = 5 
B0 <- as.matrix(B0,nrow = 1, ncol = (p+1))
sigma0 <- as.matrix(diag(1,(p+1)))


T0 = 1 #prior degrees of freedom 
D0 = 0.1 # prior scale (theta0)
# inital value for variance 

##############################################################
#Some initial set-up to Gibbs Sampler 
reps = 15000 
burn = 4000
sigma2 = 1
B = B0
horizon = 14
# May take along time to compute

# Storage matrix 
out = matrix(0, nrow = reps, ncol= (p + 2)) # Number of Beta Variables plus constant and Variance 
colnames(out) <- c('constant','beta1','beta2','beat3','beta4','sigma')
out1 <- matrix(0, nrow = reps, ncol = horizon)

# Gibbs Sampling 

gibbs_sampler <- function(X,Y,B0,sigma0,sigma2,theta0,D0,reps,out,out1){
for(i in 1:reps){
    if (i %% 1000 == 0){
    print(sprintf("Interaction: %d",i))
      }
    M = solve(solve(sigma0)+ as.numeric(1/sigma2) * t(X) %*% X) %*% 
      (solve(sigma0) %*% B0 + as.numeric(1/sigma2) * t(X) %*% Y)
         
    V = solve(solve(sigma0) + as.numeric(1/sigma2) * t(X)%*%X)
    chck = -1
    while(chck < 0){#check stability
           
      B <- M + t(rnorm(p+1)%*% chol(V))   
      # Check - Not stationary for 5 lags
      b = ar_companion_matrix(B)
      ee <- max(sapply(eigen(b)$values,abs))
      if(ee<=1){
          chck=1
          }
         }
         # Compute Residuals 
      resids <- Y - X%*% B
      T2 = T0 + T1
      D1 = D0 + t(resids) %*% resids
         
      #Keeps samples after burn period
      out[i,] <- t(matrix(c(t(B),sigma2)))
         
      #Draw From Inverse Gamma
      z0 <- rnorm(T1,1)
      z0z0 = t(z0) %*% z0
      sigma2 = D1/z0z0
         
      #Keeps samples after burn period
      out[i,] <- t(matrix(c(t(B),sigma2)))
         
      #Compute 2 year forecast
      yhat = rep(0,horizon)
      end = as.numeric(length(Y))
      yhat[1:2] = Y[(end-1):end,]
      cfactor = sqrt(sigma2)
      X_mat = c(1,rep(0,p))
  for(m in (p+1):horizon){
            for (lag in 1:p){
        #create X matrix with p lags
              X_mat[(lag+1)] = yhat[m-lag]
        }
          # Use X matrix to forecast yhat
        yhat[m] = X_mat %*% B + rnorm(1) * cfactor
        
      }
  out1[i,] <- yhat
  }
    return = list(out,out1)
  }

results1 <- gibbs_sampler(X,Y,B,sigma0,sigma2,theta0,D0,reps,out,out1)
coef <- results1[[1]][(burn+1):reps,]
forecasts <- results1[[2]][(burn+1):reps,]


#######################################################
# Calculate Postertior Mean - Column Mean of each Variable
post_medians<- colMedians(coef)
forecasts_m <- as.matrix(colMedians(forecasts))
#######################################################
# Plot Histograms of Parameters
const <- median(coef[,1])
beta1 <- median(coef[,2])
beta2 <- median(coef[,3])
beta3 <- median(coef[,4])
beta4 <- median(coef[,5])
sigma2 <- median(coef[,6])

qplot(coef[,1],geom = "histogram", bins = 45, main = 'Distribution of Constant',colour = "#FF9999" )
qplot(coef[,2],geom = "histogram", bins = 45, main = 'Distribution of Beta1',colour = "#FF9999")
qplot(coef[,3],geom = "histogram", bins = 45, main = 'Distribution of Beta2',colour = "#FF9999")
qplot(coef[,4],geom = "histogram", bins = 45, main = 'Distribution of Beta_{3}',colour = "#FF9999")
qplot(coef[,4],geom = "histogram", bins = 45, main = 'Distribution of Beta_{4}',colour = "#FF9999")
qplot(coef[,4],geom = "histogram", bins = 45, main = 'Distribution of Sigma^{2}',colour = "#FF9999")
error_bands <- colQuantiles(forecasts,probs = c(0.16,0.84))
Y_temp = cbind(Y,Y)

error_bands <- rbind(Y_temp, error_bands[3:dim(error_bands)[1],])
all <- as.matrix(c(Y[1:(length(Y)-2)],forecasts_m))
# Combine Bands and Forecast variables into one set --> definded by 
forecasts.mat <- cbind.data.frame(error_bands[,1],all, error_bands[,2]) 
names(forecasts.mat) <- c('lower','median','upper')

#Create date vector for plotting:
Date <- seq(as.Date('1961/04/01'),by = 'quarter', length.out = length(all))

# regular column binding (cbind) loses data class
data.plot <- cbind.data.frame(Date,forecasts.mat)

# puts into long format for plotting
# easiest way to plot I found was to generate
# intervals for all data and forecasts
# intervals for actual data WOULD JUST BE REPEATED as actual data

data.plot_melt <- melt(data.plot,id = 'Date')
ggplot(data.plot_melt,aes(x=Date,y=value,colour=variable)) + geom_line()

ggplot(data.plot_melt, aes(x = Date,y = value)) + geom_line(data=data.plot_melt,aes(colour=variable))

# plot with confidence intervals 
ggplot(data.plot, aes(x = Date, y = median))+ geom_line(colour = 'red') +
  geom_ribbon(data = data.plot ,
              aes(ymin = lower, ymax = upper, alpha = 0.2))

# Plot subset of data 
data_subset <- data.plot[150:241,]
data_fore <-data.plot[200:241,]
ggplot(data_subset,aes(x=Date,y=median)) + geom_line(colour = "red",lwd = 1) + geom_ribbon(data = data_fore,aes(ymin = lower,ymax = upper,colour="bands",alpha = 0.2), main = "Median Growth Rate of Canadian GDP")







